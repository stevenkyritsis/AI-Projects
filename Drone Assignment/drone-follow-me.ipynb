{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": 12,
            "id": "dc46826e",
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "0"
                        ]
                    },
                    "execution_count": 12,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "# Run this to cleanup environment if you need to rerun the code!\n",
                "import os\n",
                "os.system(\"rm *.mp4\")\n",
                "os.system(\"rm *.txt\")\n",
                "os.system(\"rm *.csv\")\n",
                "os.system(\"rm -rf result*\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "b53b6f23",
            "metadata": {},
            "source": [
                "# Drone follow me using Kalman Filters\n",
                "\n",
                "Multi-Object Tracking (MOT) is a core visual ability that humans poses to perform kinetic tasks and coordinate other tasks. The AI community has recognized the importance of MOT via a series of [competitions](https://motchallenge.net). \n",
                "\n",
                "In this assignment, the object class is `bicycle` and `car` the ability to track these objects  will be demonstrated using [Kalman Filters](https://en.wikipedia.org/wiki/Kalman_filter).  \n",
                "\n",
                "\n",
                "## Task 1: Setup your development environment and store the test video locally (10 points)\n",
                "\n",
                "Your environment must be docker based and you can use any TF2 or PT2 based docker container compatible with your environment. You can also use colab. "
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "'/home/jovyan/CS370-Assignments/Drone Assignment/Drone Tracking Video.mp4'"
                        ]
                    },
                    "execution_count": 4,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "from pytube import YouTube\n",
                "YouTube('https://www.youtube.com/watch?v=WeF4wpw7w9kWeF4wpw7w9k').streams.first().download()\n",
                "YouTube('https://www.youtube.com/watch?v=2NFwY15tRtA').streams.first().download()\n",
                "YouTube('https://www.youtube.com/watch?v=5dRramZVu2Q').streams.first().download()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "id": "2385bd73",
            "metadata": {},
            "outputs": [],
            "source": [
                "input_file1 = 'Cyclist and vehicle Tracking - 1.mp4'\n",
                "input_file2 = 'Cyclist and vehicle Tracking - 2.mp4'\n",
                "input_file3 = 'Drone Tracking Video.mp4'"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "8514b0e1",
            "metadata": {},
            "source": [
                "## Task 2: Object Detection (40 points)\n",
                "\n",
                "Perform object detection on the following videos. \n",
                "\n",
                "```{eval-rst}\n",
                ".. youtube:: https://www.youtube.com/watch?v=WeF4wpw7w9kWeF4wpw7w9k\n",
                "```\n",
                "\n",
                "```{eval-rst}\n",
                ".. youtube:: https://www.youtube.com/watch?v=2NFwY15tRtA\n",
                "```\n",
                "\n",
                "```{eval-rst}\n",
                ".. youtube:: https://www.youtube.com/watch?v=5dRramZVu2Q\n",
                "```\n",
                "Split the videos into frames and use an object detector of your choice, in a framework of your choice to detect the cyclists.  "
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "id": "a8b2b678",
            "metadata": {},
            "outputs": [],
            "source": [
                "import pandas as pd\n",
                "import numpy as np\n",
                "import cv2\n",
                "import matplotlib.pyplot as plt\n",
                "import os\n",
                "\n",
                "from glob import glob\n",
                "\n",
                "import IPython.display as ipd\n",
                "from tqdm.notebook import tqdm"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "id": "4dc14373",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load in video capture\n",
                "cap1 = cv2.VideoCapture('input_file1.mp4')\n",
                "cap2 = cv2.VideoCapture('input_file2.mp4')\n",
                "cap3 = cv2.VideoCapture('input_file3.mp4')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 11,
            "id": "af101bc6",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "0: 352x640 (no detections), 1032.6ms\n",
                        "Speed: 6.4ms preprocess, 1032.6ms inference, 12.5ms postprocess per image at shape (1, 3, 352, 640)\n",
                        "Results saved to \u001b[1mruns/detect/predict\u001b[0m\n",
                        "\n",
                        "0: 352x640 (no detections), 1074.9ms\n",
                        "Speed: 2.4ms preprocess, 1074.9ms inference, 0.2ms postprocess per image at shape (1, 3, 352, 640)\n",
                        "Results saved to \u001b[1mruns/detect/predict\u001b[0m\n",
                        "\n",
                        "0: 352x640 (no detections), 955.5ms\n",
                        "Speed: 4.6ms preprocess, 955.5ms inference, 1.6ms postprocess per image at shape (1, 3, 352, 640)\n",
                        "Results saved to \u001b[1mruns/detect/predict\u001b[0m\n",
                        "\n",
                        "0: 352x640 (no detections), 1064.8ms\n",
                        "Speed: 0.6ms preprocess, 1064.8ms inference, 0.3ms postprocess per image at shape (1, 3, 352, 640)\n",
                        "Results saved to \u001b[1mruns/detect/predict\u001b[0m\n",
                        "\n",
                        "0: 352x640 (no detections), 913.7ms\n",
                        "Speed: 0.4ms preprocess, 913.7ms inference, 0.2ms postprocess per image at shape (1, 3, 352, 640)\n",
                        "Results saved to \u001b[1mruns/detect/predict\u001b[0m\n",
                        "\n",
                        "0: 352x640 (no detections), 679.4ms\n",
                        "Speed: 1.0ms preprocess, 679.4ms inference, 0.2ms postprocess per image at shape (1, 3, 352, 640)\n",
                        "Results saved to \u001b[1mruns/detect/predict\u001b[0m\n",
                        "\n",
                        "0: 352x640 (no detections), 733.2ms\n",
                        "Speed: 1.2ms preprocess, 733.2ms inference, 0.3ms postprocess per image at shape (1, 3, 352, 640)\n",
                        "Results saved to \u001b[1mruns/detect/predict\u001b[0m\n",
                        "\n",
                        "0: 352x640 (no detections), 1061.8ms\n",
                        "Speed: 1.2ms preprocess, 1061.8ms inference, 0.2ms postprocess per image at shape (1, 3, 352, 640)\n",
                        "Results saved to \u001b[1mruns/detect/predict\u001b[0m\n",
                        "\n",
                        "0: 352x640 (no detections), 1003.5ms\n",
                        "Speed: 2.7ms preprocess, 1003.5ms inference, 0.2ms postprocess per image at shape (1, 3, 352, 640)\n",
                        "Results saved to \u001b[1mruns/detect/predict\u001b[0m\n",
                        "\n",
                        "0: 352x640 (no detections), 912.9ms\n",
                        "Speed: 4.2ms preprocess, 912.9ms inference, 0.4ms postprocess per image at shape (1, 3, 352, 640)\n",
                        "Results saved to \u001b[1mruns/detect/predict\u001b[0m\n",
                        "\n",
                        "0: 352x640 (no detections), 697.8ms\n",
                        "Speed: 1.5ms preprocess, 697.8ms inference, 0.3ms postprocess per image at shape (1, 3, 352, 640)\n",
                        "Results saved to \u001b[1mruns/detect/predict\u001b[0m\n",
                        "\n",
                        "0: 352x640 (no detections), 711.0ms\n",
                        "Speed: 0.3ms preprocess, 711.0ms inference, 0.2ms postprocess per image at shape (1, 3, 352, 640)\n",
                        "Results saved to \u001b[1mruns/detect/predict\u001b[0m\n",
                        "\n",
                        "0: 352x640 (no detections), 537.9ms\n",
                        "Speed: 5.5ms preprocess, 537.9ms inference, 0.5ms postprocess per image at shape (1, 3, 352, 640)\n",
                        "Results saved to \u001b[1mruns/detect/predict\u001b[0m\n",
                        "\n",
                        "0: 352x640 (no detections), 912.2ms\n",
                        "Speed: 1.3ms preprocess, 912.2ms inference, 0.6ms postprocess per image at shape (1, 3, 352, 640)\n",
                        "Results saved to \u001b[1mruns/detect/predict\u001b[0m\n",
                        "\n",
                        "0: 352x640 (no detections), 827.5ms\n",
                        "Speed: 2.7ms preprocess, 827.5ms inference, 0.2ms postprocess per image at shape (1, 3, 352, 640)\n",
                        "Results saved to \u001b[1mruns/detect/predict\u001b[0m\n",
                        "\n",
                        "0: 352x640 (no detections), 934.0ms\n",
                        "Speed: 4.0ms preprocess, 934.0ms inference, 0.2ms postprocess per image at shape (1, 3, 352, 640)\n",
                        "Results saved to \u001b[1mruns/detect/predict\u001b[0m\n",
                        "\n",
                        "0: 352x640 (no detections), 840.8ms\n",
                        "Speed: 4.5ms preprocess, 840.8ms inference, 0.2ms postprocess per image at shape (1, 3, 352, 640)\n",
                        "Results saved to \u001b[1mruns/detect/predict\u001b[0m\n",
                        "\n",
                        "0: 352x640 (no detections), 712.4ms\n",
                        "Speed: 1.0ms preprocess, 712.4ms inference, 0.3ms postprocess per image at shape (1, 3, 352, 640)\n",
                        "Results saved to \u001b[1mruns/detect/predict\u001b[0m\n",
                        "\n",
                        "0: 352x640 (no detections), 713.1ms\n",
                        "Speed: 2.0ms preprocess, 713.1ms inference, 0.2ms postprocess per image at shape (1, 3, 352, 640)\n",
                        "Results saved to \u001b[1mruns/detect/predict\u001b[0m\n",
                        "\n",
                        "0: 352x640 (no detections), 1015.3ms\n",
                        "Speed: 0.5ms preprocess, 1015.3ms inference, 0.2ms postprocess per image at shape (1, 3, 352, 640)\n",
                        "Results saved to \u001b[1mruns/detect/predict\u001b[0m\n",
                        "\n",
                        "0: 352x640 (no detections), 942.3ms\n",
                        "Speed: 3.1ms preprocess, 942.3ms inference, 0.2ms postprocess per image at shape (1, 3, 352, 640)\n",
                        "Results saved to \u001b[1mruns/detect/predict\u001b[0m\n",
                        "\n",
                        "0: 352x640 (no detections), 801.4ms\n",
                        "Speed: 0.5ms preprocess, 801.4ms inference, 0.2ms postprocess per image at shape (1, 3, 352, 640)\n",
                        "Results saved to \u001b[1mruns/detect/predict\u001b[0m\n",
                        "\n",
                        "0: 352x640 (no detections), 812.3ms\n",
                        "Speed: 3.8ms preprocess, 812.3ms inference, 0.2ms postprocess per image at shape (1, 3, 352, 640)\n",
                        "Results saved to \u001b[1mruns/detect/predict\u001b[0m\n",
                        "\n",
                        "0: 352x640 (no detections), 557.1ms\n",
                        "Speed: 1.2ms preprocess, 557.1ms inference, 0.4ms postprocess per image at shape (1, 3, 352, 640)\n",
                        "Results saved to \u001b[1mruns/detect/predict\u001b[0m\n",
                        "\n",
                        "0: 352x640 (no detections), 706.6ms\n",
                        "Speed: 3.5ms preprocess, 706.6ms inference, 0.2ms postprocess per image at shape (1, 3, 352, 640)\n",
                        "Results saved to \u001b[1mruns/detect/predict\u001b[0m\n",
                        "\n",
                        "0: 352x640 (no detections), 779.0ms\n",
                        "Speed: 5.2ms preprocess, 779.0ms inference, 0.4ms postprocess per image at shape (1, 3, 352, 640)\n",
                        "Results saved to \u001b[1mruns/detect/predict\u001b[0m\n",
                        "\n",
                        "0: 352x640 (no detections), 920.6ms\n",
                        "Speed: 3.4ms preprocess, 920.6ms inference, 0.9ms postprocess per image at shape (1, 3, 352, 640)\n",
                        "Results saved to \u001b[1mruns/detect/predict\u001b[0m\n",
                        "\n",
                        "0: 352x640 (no detections), 945.4ms\n",
                        "Speed: 0.2ms preprocess, 945.4ms inference, 0.3ms postprocess per image at shape (1, 3, 352, 640)\n",
                        "Results saved to \u001b[1mruns/detect/predict\u001b[0m\n",
                        "\n",
                        "0: 352x640 (no detections), 931.9ms\n",
                        "Speed: 4.4ms preprocess, 931.9ms inference, 0.2ms postprocess per image at shape (1, 3, 352, 640)\n",
                        "Results saved to \u001b[1mruns/detect/predict\u001b[0m\n",
                        "\n",
                        "0: 352x640 (no detections), 521.9ms\n",
                        "Speed: 1.1ms preprocess, 521.9ms inference, 0.2ms postprocess per image at shape (1, 3, 352, 640)\n",
                        "Results saved to \u001b[1mruns/detect/predict\u001b[0m\n",
                        "\n",
                        "0: 352x640 (no detections), 634.0ms\n",
                        "Speed: 0.9ms preprocess, 634.0ms inference, 0.2ms postprocess per image at shape (1, 3, 352, 640)\n",
                        "Results saved to \u001b[1mruns/detect/predict\u001b[0m\n",
                        "\n",
                        "0: 352x640 (no detections), 768.6ms\n",
                        "Speed: 2.1ms preprocess, 768.6ms inference, 0.2ms postprocess per image at shape (1, 3, 352, 640)\n",
                        "Results saved to \u001b[1mruns/detect/predict\u001b[0m\n",
                        "\n",
                        "0: 352x640 (no detections), 944.2ms\n",
                        "Speed: 7.1ms preprocess, 944.2ms inference, 0.2ms postprocess per image at shape (1, 3, 352, 640)\n",
                        "Results saved to \u001b[1mruns/detect/predict\u001b[0m\n",
                        "\n",
                        "0: 352x640 (no detections), 848.4ms\n",
                        "Speed: 0.7ms preprocess, 848.4ms inference, 0.4ms postprocess per image at shape (1, 3, 352, 640)\n",
                        "Results saved to \u001b[1mruns/detect/predict\u001b[0m\n",
                        "\n",
                        "0: 352x640 (no detections), 895.3ms\n",
                        "Speed: 2.4ms preprocess, 895.3ms inference, 0.2ms postprocess per image at shape (1, 3, 352, 640)\n",
                        "Results saved to \u001b[1mruns/detect/predict\u001b[0m\n",
                        "\n",
                        "0: 352x640 (no detections), 672.7ms\n",
                        "Speed: 4.8ms preprocess, 672.7ms inference, 0.2ms postprocess per image at shape (1, 3, 352, 640)\n",
                        "Results saved to \u001b[1mruns/detect/predict\u001b[0m\n",
                        "\n",
                        "0: 352x640 (no detections), 558.2ms\n",
                        "Speed: 1.6ms preprocess, 558.2ms inference, 0.4ms postprocess per image at shape (1, 3, 352, 640)\n",
                        "Results saved to \u001b[1mruns/detect/predict\u001b[0m\n",
                        "\n",
                        "0: 352x640 (no detections), 633.1ms\n",
                        "Speed: 0.3ms preprocess, 633.1ms inference, 0.2ms postprocess per image at shape (1, 3, 352, 640)\n",
                        "Results saved to \u001b[1mruns/detect/predict\u001b[0m\n",
                        "\n",
                        "0: 352x640 (no detections), 840.0ms\n",
                        "Speed: 10.0ms preprocess, 840.0ms inference, 0.2ms postprocess per image at shape (1, 3, 352, 640)\n",
                        "Results saved to \u001b[1mruns/detect/predict\u001b[0m\n",
                        "\n",
                        "0: 352x640 (no detections), 803.7ms\n",
                        "Speed: 3.9ms preprocess, 803.7ms inference, 0.2ms postprocess per image at shape (1, 3, 352, 640)\n",
                        "Results saved to \u001b[1mruns/detect/predict\u001b[0m\n",
                        "\n",
                        "0: 352x640 (no detections), 923.5ms\n",
                        "Speed: 4.6ms preprocess, 923.5ms inference, 0.4ms postprocess per image at shape (1, 3, 352, 640)\n",
                        "Results saved to \u001b[1mruns/detect/predict\u001b[0m\n",
                        "\n",
                        "0: 352x640 (no detections), 814.7ms\n",
                        "Speed: 0.9ms preprocess, 814.7ms inference, 0.2ms postprocess per image at shape (1, 3, 352, 640)\n",
                        "Results saved to \u001b[1mruns/detect/predict\u001b[0m\n",
                        "\n",
                        "0: 352x640 (no detections), 562.6ms\n",
                        "Speed: 0.4ms preprocess, 562.6ms inference, 0.2ms postprocess per image at shape (1, 3, 352, 640)\n",
                        "Results saved to \u001b[1mruns/detect/predict\u001b[0m\n",
                        "\n",
                        "0: 352x640 (no detections), 670.5ms\n",
                        "Speed: 3.0ms preprocess, 670.5ms inference, 0.5ms postprocess per image at shape (1, 3, 352, 640)\n",
                        "Results saved to \u001b[1mruns/detect/predict\u001b[0m\n",
                        "\n",
                        "0: 352x640 (no detections), 870.5ms\n",
                        "Speed: 4.3ms preprocess, 870.5ms inference, 0.2ms postprocess per image at shape (1, 3, 352, 640)\n",
                        "Results saved to \u001b[1mruns/detect/predict\u001b[0m\n",
                        "\n",
                        "0: 352x640 (no detections), 960.7ms\n",
                        "Speed: 16.4ms preprocess, 960.7ms inference, 0.2ms postprocess per image at shape (1, 3, 352, 640)\n",
                        "Results saved to \u001b[1mruns/detect/predict\u001b[0m\n",
                        "\n",
                        "0: 352x640 (no detections), 900.8ms\n",
                        "Speed: 1.4ms preprocess, 900.8ms inference, 0.2ms postprocess per image at shape (1, 3, 352, 640)\n",
                        "Results saved to \u001b[1mruns/detect/predict\u001b[0m\n",
                        "\n",
                        "0: 352x640 (no detections), 909.9ms\n",
                        "Speed: 2.4ms preprocess, 909.9ms inference, 0.2ms postprocess per image at shape (1, 3, 352, 640)\n",
                        "Results saved to \u001b[1mruns/detect/predict\u001b[0m\n",
                        "\n",
                        "0: 352x640 (no detections), 545.0ms\n",
                        "Speed: 2.3ms preprocess, 545.0ms inference, 0.4ms postprocess per image at shape (1, 3, 352, 640)\n",
                        "Results saved to \u001b[1mruns/detect/predict\u001b[0m\n",
                        "\n",
                        "0: 352x640 (no detections), 590.7ms\n",
                        "Speed: 0.4ms preprocess, 590.7ms inference, 0.4ms postprocess per image at shape (1, 3, 352, 640)\n",
                        "Results saved to \u001b[1mruns/detect/predict\u001b[0m\n",
                        "\n",
                        "0: 352x640 (no detections), 701.5ms\n",
                        "Speed: 4.3ms preprocess, 701.5ms inference, 0.2ms postprocess per image at shape (1, 3, 352, 640)\n",
                        "Results saved to \u001b[1mruns/detect/predict\u001b[0m\n",
                        "\n",
                        "0: 352x640 (no detections), 815.9ms\n",
                        "Speed: 2.4ms preprocess, 815.9ms inference, 0.5ms postprocess per image at shape (1, 3, 352, 640)\n",
                        "Results saved to \u001b[1mruns/detect/predict\u001b[0m\n",
                        "\n",
                        "0: 352x640 (no detections), 762.1ms\n",
                        "Speed: 1.9ms preprocess, 762.1ms inference, 0.7ms postprocess per image at shape (1, 3, 352, 640)\n",
                        "Results saved to \u001b[1mruns/detect/predict\u001b[0m\n",
                        "\n",
                        "0: 352x640 (no detections), 821.8ms\n",
                        "Speed: 2.0ms preprocess, 821.8ms inference, 0.2ms postprocess per image at shape (1, 3, 352, 640)\n",
                        "Results saved to \u001b[1mruns/detect/predict\u001b[0m\n",
                        "\n",
                        "0: 352x640 (no detections), 746.4ms\n",
                        "Speed: 0.5ms preprocess, 746.4ms inference, 1.2ms postprocess per image at shape (1, 3, 352, 640)\n",
                        "Results saved to \u001b[1mruns/detect/predict\u001b[0m\n",
                        "\n",
                        "0: 352x640 (no detections), 619.8ms\n",
                        "Speed: 1.6ms preprocess, 619.8ms inference, 0.2ms postprocess per image at shape (1, 3, 352, 640)\n",
                        "Results saved to \u001b[1mruns/detect/predict\u001b[0m\n",
                        "\n",
                        "0: 352x640 (no detections), 568.2ms\n",
                        "Speed: 0.5ms preprocess, 568.2ms inference, 0.5ms postprocess per image at shape (1, 3, 352, 640)\n",
                        "Results saved to \u001b[1mruns/detect/predict\u001b[0m\n",
                        "\n",
                        "0: 352x640 (no detections), 1006.1ms\n",
                        "Speed: 1.7ms preprocess, 1006.1ms inference, 0.8ms postprocess per image at shape (1, 3, 352, 640)\n",
                        "Results saved to \u001b[1mruns/detect/predict\u001b[0m\n",
                        "\n",
                        "0: 352x640 (no detections), 877.8ms\n",
                        "Speed: 3.1ms preprocess, 877.8ms inference, 0.2ms postprocess per image at shape (1, 3, 352, 640)\n",
                        "Results saved to \u001b[1mruns/detect/predict\u001b[0m\n",
                        "\n",
                        "0: 352x640 (no detections), 895.0ms\n",
                        "Speed: 0.9ms preprocess, 895.0ms inference, 0.2ms postprocess per image at shape (1, 3, 352, 640)\n",
                        "Results saved to \u001b[1mruns/detect/predict\u001b[0m\n",
                        "\n",
                        "0: 352x640 (no detections), 1029.6ms\n",
                        "Speed: 1.5ms preprocess, 1029.6ms inference, 0.2ms postprocess per image at shape (1, 3, 352, 640)\n",
                        "Results saved to \u001b[1mruns/detect/predict\u001b[0m\n",
                        "\n",
                        "0: 352x640 (no detections), 656.2ms\n",
                        "Speed: 2.4ms preprocess, 656.2ms inference, 0.2ms postprocess per image at shape (1, 3, 352, 640)\n",
                        "Results saved to \u001b[1mruns/detect/predict\u001b[0m\n",
                        "\n",
                        "0: 352x640 (no detections), 1829.3ms\n",
                        "Speed: 0.3ms preprocess, 1829.3ms inference, 0.2ms postprocess per image at shape (1, 3, 352, 640)\n",
                        "Results saved to \u001b[1mruns/detect/predict\u001b[0m\n",
                        "\n",
                        "0: 352x640 (no detections), 1028.7ms\n",
                        "Speed: 3.5ms preprocess, 1028.7ms inference, 0.2ms postprocess per image at shape (1, 3, 352, 640)\n",
                        "Results saved to \u001b[1mruns/detect/predict\u001b[0m\n",
                        "\n",
                        "0: 352x640 (no detections), 989.4ms\n",
                        "Speed: 3.2ms preprocess, 989.4ms inference, 0.2ms postprocess per image at shape (1, 3, 352, 640)\n",
                        "Results saved to \u001b[1mruns/detect/predict\u001b[0m\n",
                        "\n",
                        "0: 352x640 (no detections), 824.5ms\n",
                        "Speed: 7.9ms preprocess, 824.5ms inference, 0.3ms postprocess per image at shape (1, 3, 352, 640)\n",
                        "Results saved to \u001b[1mruns/detect/predict\u001b[0m\n",
                        "\n",
                        "0: 352x640 (no detections), 820.7ms\n",
                        "Speed: 2.4ms preprocess, 820.7ms inference, 0.2ms postprocess per image at shape (1, 3, 352, 640)\n",
                        "Results saved to \u001b[1mruns/detect/predict\u001b[0m\n",
                        "\n",
                        "0: 352x640 (no detections), 1004.0ms\n",
                        "Speed: 0.3ms preprocess, 1004.0ms inference, 2.2ms postprocess per image at shape (1, 3, 352, 640)\n",
                        "Results saved to \u001b[1mruns/detect/predict\u001b[0m\n",
                        "\n",
                        "0: 352x640 (no detections), 1197.4ms\n",
                        "Speed: 0.5ms preprocess, 1197.4ms inference, 0.3ms postprocess per image at shape (1, 3, 352, 640)\n",
                        "Results saved to \u001b[1mruns/detect/predict\u001b[0m\n",
                        "\n",
                        "0: 352x640 (no detections), 967.8ms\n",
                        "Speed: 5.7ms preprocess, 967.8ms inference, 0.3ms postprocess per image at shape (1, 3, 352, 640)\n",
                        "Results saved to \u001b[1mruns/detect/predict\u001b[0m\n",
                        "\n",
                        "0: 352x640 (no detections), 702.3ms\n",
                        "Speed: 0.5ms preprocess, 702.3ms inference, 0.2ms postprocess per image at shape (1, 3, 352, 640)\n",
                        "Results saved to \u001b[1mruns/detect/predict\u001b[0m\n",
                        "\n",
                        "0: 352x640 (no detections), 599.7ms\n",
                        "Speed: 1.3ms preprocess, 599.7ms inference, 0.6ms postprocess per image at shape (1, 3, 352, 640)\n",
                        "Results saved to \u001b[1mruns/detect/predict\u001b[0m\n",
                        "\n",
                        "0: 352x640 (no detections), 806.9ms\n",
                        "Speed: 1.5ms preprocess, 806.9ms inference, 0.5ms postprocess per image at shape (1, 3, 352, 640)\n",
                        "Results saved to \u001b[1mruns/detect/predict\u001b[0m\n",
                        "\n",
                        "0: 352x640 1 car, 1212.6ms\n",
                        "Speed: 13.5ms preprocess, 1212.6ms inference, 6.2ms postprocess per image at shape (1, 3, 352, 640)\n",
                        "Results saved to \u001b[1mruns/detect/predict\u001b[0m\n",
                        "\n",
                        "0: 352x640 (no detections), 1012.4ms\n",
                        "Speed: 0.9ms preprocess, 1012.4ms inference, 0.2ms postprocess per image at shape (1, 3, 352, 640)\n",
                        "Results saved to \u001b[1mruns/detect/predict\u001b[0m\n",
                        "\n",
                        "0: 352x640 (no detections), 974.6ms\n",
                        "Speed: 0.4ms preprocess, 974.6ms inference, 0.2ms postprocess per image at shape (1, 3, 352, 640)\n",
                        "Results saved to \u001b[1mruns/detect/predict\u001b[0m\n",
                        "\n",
                        "0: 352x640 1 car, 745.6ms\n",
                        "Speed: 2.1ms preprocess, 745.6ms inference, 0.4ms postprocess per image at shape (1, 3, 352, 640)\n",
                        "Results saved to \u001b[1mruns/detect/predict\u001b[0m\n",
                        "\n",
                        "0: 352x640 1 truck, 717.3ms\n",
                        "Speed: 0.5ms preprocess, 717.3ms inference, 0.3ms postprocess per image at shape (1, 3, 352, 640)\n",
                        "Results saved to \u001b[1mruns/detect/predict\u001b[0m\n",
                        "\n",
                        "0: 352x640 1 truck, 944.6ms\n",
                        "Speed: 1.0ms preprocess, 944.6ms inference, 0.6ms postprocess per image at shape (1, 3, 352, 640)\n",
                        "Results saved to \u001b[1mruns/detect/predict\u001b[0m\n",
                        "\n",
                        "0: 352x640 1 car, 1166.6ms\n",
                        "Speed: 4.8ms preprocess, 1166.6ms inference, 0.4ms postprocess per image at shape (1, 3, 352, 640)\n",
                        "Results saved to \u001b[1mruns/detect/predict\u001b[0m\n",
                        "\n",
                        "0: 352x640 1 truck, 1447.5ms\n",
                        "Speed: 0.3ms preprocess, 1447.5ms inference, 0.4ms postprocess per image at shape (1, 3, 352, 640)\n",
                        "Results saved to \u001b[1mruns/detect/predict\u001b[0m\n",
                        "\n",
                        "0: 352x640 1 truck, 880.8ms\n",
                        "Speed: 13.1ms preprocess, 880.8ms inference, 0.3ms postprocess per image at shape (1, 3, 352, 640)\n",
                        "Results saved to \u001b[1mruns/detect/predict\u001b[0m\n",
                        "\n",
                        "0: 352x640 1 truck, 656.0ms\n",
                        "Speed: 1.1ms preprocess, 656.0ms inference, 0.4ms postprocess per image at shape (1, 3, 352, 640)\n",
                        "Results saved to \u001b[1mruns/detect/predict\u001b[0m\n",
                        "\n",
                        "0: 352x640 1 car, 1002.3ms\n",
                        "Speed: 3.0ms preprocess, 1002.3ms inference, 0.4ms postprocess per image at shape (1, 3, 352, 640)\n",
                        "Results saved to \u001b[1mruns/detect/predict\u001b[0m\n",
                        "\n",
                        "0: 352x640 1 car, 828.9ms\n",
                        "Speed: 2.0ms preprocess, 828.9ms inference, 0.3ms postprocess per image at shape (1, 3, 352, 640)\n",
                        "Results saved to \u001b[1mruns/detect/predict\u001b[0m\n",
                        "\n",
                        "0: 352x640 1 car, 884.4ms\n",
                        "Speed: 2.2ms preprocess, 884.4ms inference, 0.4ms postprocess per image at shape (1, 3, 352, 640)\n",
                        "Results saved to \u001b[1mruns/detect/predict\u001b[0m\n",
                        "\n",
                        "0: 352x640 1 car, 781.6ms\n",
                        "Speed: 2.1ms preprocess, 781.6ms inference, 0.3ms postprocess per image at shape (1, 3, 352, 640)\n",
                        "Results saved to \u001b[1mruns/detect/predict\u001b[0m\n",
                        "\n",
                        "0: 352x640 1 car, 757.5ms\n",
                        "Speed: 1.3ms preprocess, 757.5ms inference, 0.3ms postprocess per image at shape (1, 3, 352, 640)\n",
                        "Results saved to \u001b[1mruns/detect/predict\u001b[0m\n",
                        "\n",
                        "0: 352x640 1 car, 1 truck, 581.5ms\n",
                        "Speed: 0.7ms preprocess, 581.5ms inference, 0.3ms postprocess per image at shape (1, 3, 352, 640)\n",
                        "Results saved to \u001b[1mruns/detect/predict\u001b[0m\n",
                        "\n",
                        "0: 352x640 1 car, 948.7ms\n",
                        "Speed: 2.7ms preprocess, 948.7ms inference, 0.3ms postprocess per image at shape (1, 3, 352, 640)\n",
                        "Results saved to \u001b[1mruns/detect/predict\u001b[0m\n",
                        "\n",
                        "0: 352x640 1 car, 933.7ms\n",
                        "Speed: 7.9ms preprocess, 933.7ms inference, 0.4ms postprocess per image at shape (1, 3, 352, 640)\n",
                        "Results saved to \u001b[1mruns/detect/predict\u001b[0m\n",
                        "\n",
                        "0: 352x640 1 car, 896.8ms\n",
                        "Speed: 0.8ms preprocess, 896.8ms inference, 0.3ms postprocess per image at shape (1, 3, 352, 640)\n",
                        "Results saved to \u001b[1mruns/detect/predict\u001b[0m\n",
                        "\n",
                        "0: 352x640 1 car, 842.9ms\n",
                        "Speed: 0.3ms preprocess, 842.9ms inference, 0.3ms postprocess per image at shape (1, 3, 352, 640)\n",
                        "Results saved to \u001b[1mruns/detect/predict\u001b[0m\n",
                        "\n",
                        "0: 352x640 1 car, 621.6ms\n",
                        "Speed: 0.3ms preprocess, 621.6ms inference, 0.3ms postprocess per image at shape (1, 3, 352, 640)\n",
                        "Results saved to \u001b[1mruns/detect/predict\u001b[0m\n",
                        "\n",
                        "0: 352x640 1 car, 1 truck, 836.9ms\n",
                        "Speed: 1.2ms preprocess, 836.9ms inference, 0.3ms postprocess per image at shape (1, 3, 352, 640)\n",
                        "Results saved to \u001b[1mruns/detect/predict\u001b[0m\n",
                        "\n",
                        "0: 352x640 1 car, 1 truck, 827.7ms\n",
                        "Speed: 1.6ms preprocess, 827.7ms inference, 0.3ms postprocess per image at shape (1, 3, 352, 640)\n",
                        "Results saved to \u001b[1mruns/detect/predict\u001b[0m\n",
                        "\n",
                        "0: 352x640 1 car, 966.3ms\n",
                        "Speed: 7.2ms preprocess, 966.3ms inference, 1.7ms postprocess per image at shape (1, 3, 352, 640)\n",
                        "Results saved to \u001b[1mruns/detect/predict\u001b[0m\n",
                        "\n",
                        "0: 352x640 1 car, 981.7ms\n",
                        "Speed: 4.8ms preprocess, 981.7ms inference, 0.4ms postprocess per image at shape (1, 3, 352, 640)\n",
                        "Results saved to \u001b[1mruns/detect/predict\u001b[0m\n",
                        "\n",
                        "0: 352x640 1 car, 723.1ms\n",
                        "Speed: 5.5ms preprocess, 723.1ms inference, 0.7ms postprocess per image at shape (1, 3, 352, 640)\n",
                        "Results saved to \u001b[1mruns/detect/predict\u001b[0m\n",
                        "\n",
                        "0: 352x640 1 car, 547.9ms\n",
                        "Speed: 1.9ms preprocess, 547.9ms inference, 0.4ms postprocess per image at shape (1, 3, 352, 640)\n",
                        "Results saved to \u001b[1mruns/detect/predict\u001b[0m\n",
                        "\n",
                        "0: 352x640 1 car, 1 truck, 750.9ms\n",
                        "Speed: 0.2ms preprocess, 750.9ms inference, 0.3ms postprocess per image at shape (1, 3, 352, 640)\n",
                        "Results saved to \u001b[1mruns/detect/predict\u001b[0m\n",
                        "\n",
                        "0: 352x640 1 car, 1 truck, 1030.4ms\n",
                        "Speed: 1.2ms preprocess, 1030.4ms inference, 0.3ms postprocess per image at shape (1, 3, 352, 640)\n",
                        "Results saved to \u001b[1mruns/detect/predict\u001b[0m\n",
                        "\n",
                        "0: 352x640 1 car, 917.8ms\n",
                        "Speed: 3.4ms preprocess, 917.8ms inference, 0.3ms postprocess per image at shape (1, 3, 352, 640)\n",
                        "Results saved to \u001b[1mruns/detect/predict\u001b[0m\n",
                        "\n",
                        "0: 352x640 1 car, 1 truck, 941.0ms\n",
                        "Speed: 0.6ms preprocess, 941.0ms inference, 1.4ms postprocess per image at shape (1, 3, 352, 640)\n",
                        "Results saved to \u001b[1mruns/detect/predict\u001b[0m\n",
                        "\n",
                        "0: 352x640 1 car, 765.1ms\n",
                        "Speed: 5.1ms preprocess, 765.1ms inference, 0.3ms postprocess per image at shape (1, 3, 352, 640)\n",
                        "Results saved to \u001b[1mruns/detect/predict\u001b[0m\n",
                        "\n",
                        "0: 352x640 1 car, 555.0ms\n",
                        "Speed: 6.0ms preprocess, 555.0ms inference, 0.3ms postprocess per image at shape (1, 3, 352, 640)\n",
                        "Results saved to \u001b[1mruns/detect/predict\u001b[0m\n",
                        "\n",
                        "0: 352x640 1 car, 1 truck, 688.8ms\n",
                        "Speed: 0.2ms preprocess, 688.8ms inference, 0.3ms postprocess per image at shape (1, 3, 352, 640)\n",
                        "Results saved to \u001b[1mruns/detect/predict\u001b[0m\n",
                        "\n",
                        "0: 352x640 1 truck, 917.7ms\n",
                        "Speed: 1.0ms preprocess, 917.7ms inference, 0.7ms postprocess per image at shape (1, 3, 352, 640)\n",
                        "Results saved to \u001b[1mruns/detect/predict\u001b[0m\n",
                        "\n",
                        "0: 352x640 1 car, 949.8ms\n",
                        "Speed: 2.4ms preprocess, 949.8ms inference, 0.3ms postprocess per image at shape (1, 3, 352, 640)\n",
                        "Results saved to \u001b[1mruns/detect/predict\u001b[0m\n",
                        "\n",
                        "0: 352x640 1 car, 992.9ms\n",
                        "Speed: 4.3ms preprocess, 992.9ms inference, 0.4ms postprocess per image at shape (1, 3, 352, 640)\n",
                        "Results saved to \u001b[1mruns/detect/predict\u001b[0m\n",
                        "\n",
                        "0: 352x640 1 car, 855.0ms\n",
                        "Speed: 5.6ms preprocess, 855.0ms inference, 0.3ms postprocess per image at shape (1, 3, 352, 640)\n",
                        "Results saved to \u001b[1mruns/detect/predict\u001b[0m\n",
                        "\n",
                        "0: 352x640 1 car, 839.9ms\n",
                        "Speed: 2.6ms preprocess, 839.9ms inference, 2.2ms postprocess per image at shape (1, 3, 352, 640)\n",
                        "Results saved to \u001b[1mruns/detect/predict\u001b[0m\n",
                        "\n",
                        "0: 352x640 1 car, 674.0ms\n",
                        "Speed: 2.9ms preprocess, 674.0ms inference, 6.6ms postprocess per image at shape (1, 3, 352, 640)\n",
                        "Results saved to \u001b[1mruns/detect/predict\u001b[0m\n",
                        "\n",
                        "0: 352x640 (no detections), 954.4ms\n",
                        "Speed: 0.2ms preprocess, 954.4ms inference, 0.2ms postprocess per image at shape (1, 3, 352, 640)\n",
                        "Results saved to \u001b[1mruns/detect/predict\u001b[0m\n",
                        "\n",
                        "0: 352x640 (no detections), 929.4ms\n",
                        "Speed: 0.4ms preprocess, 929.4ms inference, 0.3ms postprocess per image at shape (1, 3, 352, 640)\n",
                        "Results saved to \u001b[1mruns/detect/predict\u001b[0m\n",
                        "\n",
                        "0: 352x640 1 car, 892.9ms\n",
                        "Speed: 6.2ms preprocess, 892.9ms inference, 0.3ms postprocess per image at shape (1, 3, 352, 640)\n",
                        "Results saved to \u001b[1mruns/detect/predict\u001b[0m\n",
                        "\n",
                        "0: 352x640 (no detections), 786.1ms\n",
                        "Speed: 0.6ms preprocess, 786.1ms inference, 0.4ms postprocess per image at shape (1, 3, 352, 640)\n",
                        "Results saved to \u001b[1mruns/detect/predict\u001b[0m\n",
                        "\n",
                        "0: 352x640 (no detections), 699.2ms\n",
                        "Speed: 0.3ms preprocess, 699.2ms inference, 0.6ms postprocess per image at shape (1, 3, 352, 640)\n",
                        "Results saved to \u001b[1mruns/detect/predict\u001b[0m\n",
                        "\n",
                        "0: 352x640 (no detections), 1078.4ms\n",
                        "Speed: 6.9ms preprocess, 1078.4ms inference, 2.0ms postprocess per image at shape (1, 3, 352, 640)\n",
                        "Results saved to \u001b[1mruns/detect/predict\u001b[0m\n",
                        "\n"
                    ]
                },
                {
                    "ename": "KeyboardInterrupt",
                    "evalue": "",
                    "output_type": "error",
                    "traceback": [
                        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
                        "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
                        "Cell \u001b[0;32mIn[11], line 35\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;66;03m#if frame % 10 == 0:        \u001b[39;00m\n\u001b[1;32m     32\u001b[0m \n\u001b[1;32m     33\u001b[0m     \u001b[38;5;66;03m#results = model(frame, device=\"mps\")\u001b[39;00m\n\u001b[1;32m     34\u001b[0m     \u001b[38;5;66;03m# use mps for macbook and device=\"0\" for windows\u001b[39;00m\n\u001b[0;32m---> 35\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msave\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmps\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.4\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m result \u001b[38;5;129;01min\u001b[39;00m results:\n\u001b[1;32m     38\u001b[0m     boxes \u001b[38;5;241m=\u001b[39m result\u001b[38;5;241m.\u001b[39mboxes  \u001b[38;5;66;03m# Boxes object for bounding box outputs\u001b[39;00m\n",
                        "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/ultralytics/engine/model.py:429\u001b[0m, in \u001b[0;36mModel.predict\u001b[0;34m(self, source, stream, predictor, **kwargs)\u001b[0m\n\u001b[1;32m    427\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m prompts \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredictor, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mset_prompts\u001b[39m\u001b[38;5;124m\"\u001b[39m):  \u001b[38;5;66;03m# for SAM-type models\u001b[39;00m\n\u001b[1;32m    428\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredictor\u001b[38;5;241m.\u001b[39mset_prompts(prompts)\n\u001b[0;32m--> 429\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredictor\u001b[38;5;241m.\u001b[39mpredict_cli(source\u001b[38;5;241m=\u001b[39msource) \u001b[38;5;28;01mif\u001b[39;00m is_cli \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredictor\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m)\u001b[49m\n",
                        "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/ultralytics/engine/predictor.py:205\u001b[0m, in \u001b[0;36mBasePredictor.__call__\u001b[0;34m(self, source, model, stream, *args, **kwargs)\u001b[0m\n\u001b[1;32m    203\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream_inference(source, model, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    204\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 205\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream_inference(source, model, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs))\n",
                        "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/utils/_contextlib.py:35\u001b[0m, in \u001b[0;36m_wrap_generator.<locals>.generator_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     33\u001b[0m     \u001b[38;5;66;03m# Issuing `None` to a generator fires it up\u001b[39;00m\n\u001b[1;32m     34\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m---> 35\u001b[0m         response \u001b[38;5;241m=\u001b[39m gen\u001b[38;5;241m.\u001b[39msend(\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m     37\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m     38\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     39\u001b[0m             \u001b[38;5;66;03m# Forward the response to our caller and get its next request\u001b[39;00m\n",
                        "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/ultralytics/engine/predictor.py:284\u001b[0m, in \u001b[0;36mBasePredictor.stream_inference\u001b[0;34m(self, source, model, *args, **kwargs)\u001b[0m\n\u001b[1;32m    282\u001b[0m \u001b[38;5;66;03m# Inference\u001b[39;00m\n\u001b[1;32m    283\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m profilers[\u001b[38;5;241m1\u001b[39m]:\n\u001b[0;32m--> 284\u001b[0m     preds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minference\u001b[49m\u001b[43m(\u001b[49m\u001b[43mim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    285\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39membed:\n\u001b[1;32m    286\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m [preds] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(preds, torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;28;01melse\u001b[39;00m preds  \u001b[38;5;66;03m# yield embedding tensors\u001b[39;00m\n",
                        "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/ultralytics/engine/predictor.py:141\u001b[0m, in \u001b[0;36mBasePredictor.inference\u001b[0;34m(self, im, *args, **kwargs)\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Runs inference on a given image using the specified model and arguments.\"\"\"\u001b[39;00m\n\u001b[1;32m    136\u001b[0m visualize \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    137\u001b[0m     increment_path(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msave_dir \u001b[38;5;241m/\u001b[39m Path(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m])\u001b[38;5;241m.\u001b[39mstem, mkdir\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    138\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mvisualize \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msource_type\u001b[38;5;241m.\u001b[39mtensor)\n\u001b[1;32m    139\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    140\u001b[0m )\n\u001b[0;32m--> 141\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maugment\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maugment\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvisualize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvisualize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membed\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
                        "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
                        "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
                        "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/ultralytics/nn/autobackend.py:384\u001b[0m, in \u001b[0;36mAutoBackend.forward\u001b[0;34m(self, im, augment, visualize, embed)\u001b[0m\n\u001b[1;32m    381\u001b[0m     im \u001b[38;5;241m=\u001b[39m im\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# torch BCHW to numpy BHWC shape(1,320,192,3)\u001b[39;00m\n\u001b[1;32m    383\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpt \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnn_module:  \u001b[38;5;66;03m# PyTorch\u001b[39;00m\n\u001b[0;32m--> 384\u001b[0m     y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maugment\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maugment\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvisualize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvisualize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43membed\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    385\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjit:  \u001b[38;5;66;03m# TorchScript\u001b[39;00m\n\u001b[1;32m    386\u001b[0m     y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel(im)\n",
                        "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
                        "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
                        "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/ultralytics/nn/tasks.py:83\u001b[0m, in \u001b[0;36mBaseModel.forward\u001b[0;34m(self, x, *args, **kwargs)\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mdict\u001b[39m):  \u001b[38;5;66;03m# for cases of training and validating while training.\u001b[39;00m\n\u001b[1;32m     82\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss(x, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m---> 83\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
                        "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/ultralytics/nn/tasks.py:101\u001b[0m, in \u001b[0;36mBaseModel.predict\u001b[0;34m(self, x, profile, visualize, augment, embed)\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m augment:\n\u001b[1;32m    100\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_predict_augment(x)\n\u001b[0;32m--> 101\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_predict_once\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprofile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvisualize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membed\u001b[49m\u001b[43m)\u001b[49m\n",
                        "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/ultralytics/nn/tasks.py:122\u001b[0m, in \u001b[0;36mBaseModel._predict_once\u001b[0;34m(self, x, profile, visualize, embed)\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m profile:\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_profile_one_layer(m, x, dt)\n\u001b[0;32m--> 122\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[43mm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# run\u001b[39;00m\n\u001b[1;32m    123\u001b[0m y\u001b[38;5;241m.\u001b[39mappend(x \u001b[38;5;28;01mif\u001b[39;00m m\u001b[38;5;241m.\u001b[39mi \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msave \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)  \u001b[38;5;66;03m# save output\u001b[39;00m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m visualize:\n",
                        "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
                        "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
                        "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/ultralytics/nn/modules/block.py:226\u001b[0m, in \u001b[0;36mC2f.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    224\u001b[0m y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcv1(x)\u001b[38;5;241m.\u001b[39mchunk(\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m    225\u001b[0m y\u001b[38;5;241m.\u001b[39mextend(m(y[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]) \u001b[38;5;28;01mfor\u001b[39;00m m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mm)\n\u001b[0;32m--> 226\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcv2(\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m)\n",
                        "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
                    ]
                }
            ],
            "source": [
                "from ultralytics import YOLO\n",
                "import csv\n",
                "\n",
                "if os.path.isdir(\"./result1\") != True:\n",
                "    os.mkdir(\"./result1\")\n",
                "\n",
                "# Load the model\n",
                "# mac = yolov8n.pt\n",
                "# windows = yolov8x.pt\n",
                "model = YOLO('yolov8x.pt')\n",
                "\n",
                "cap = cv2.VideoCapture(input_file1)\n",
                "n_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
                "\n",
                "# Get video fps\n",
                "fps = cap.get(cv2.CAP_PROP_FPS)\n",
                "\n",
                "# Open CSV file for writing\n",
                "csv_filename = \"output.csv\"\n",
                "csv_file = open(csv_filename, \"w\", newline=\"\")\n",
                "csv_writer = csv.writer(csv_file)\n",
                "csv_writer.writerow([\"vidId\", \"frameNum\", \"timestamp\", \"detectedObjId\", \"detectedObjClass\", \"confidence\", \"bbox_width\", \"bbox_height\"])\n",
                "    \n",
                "minute = 0\n",
                "sec = 0\n",
                "    \n",
                "for frame in range(n_frames):\n",
                "    ret, img = cap.read()\n",
                "    if not ret:\n",
                "        break\n",
                "    #if frame % 10 == 0:        \n",
                "\n",
                "        #results = model(frame, device=\"mps\")\n",
                "        # use mps for macbook and device=\"0\" for windows\n",
                "    results = model.predict(source=img, save=True, device=\"mps\", conf=0.4)\n",
                "\n",
                "    for result in results:\n",
                "        boxes = result.boxes  # Boxes object for bounding box outputs\n",
                "        masks = result.masks  # Masks object for segmentation masks outputs\n",
                "        keypoints = result.keypoints  # Keypoints object for pose outputs\n",
                "        probs = result.probs  # Probs object for classification outputs\n",
                "        result.save(filename=f'./result1/result{frame}.jpg')  # save to disk\n",
                "        result.save_txt('new.txt', True)\n",
                "        #csv_writer.writerow([\"output1.mp4\", f'{frame}', (frame / fps), result[4], result[3], result[4], result[1], result[2]])\n",
                "    \n",
                "    # timestamp creation\n",
                "    if frame % 24 == 0:\n",
                "        sec += 1\n",
                "        if sec % 60 == 0: #and sec > 0:\n",
                "            sec = 0\n",
                "            minute += 1\n",
                "        \n",
                "        # testing the timestamp\n",
                "        #print(f'{minute}:{sec}')\n",
                "\n",
                "\n",
                "# Close CSV file\n",
                "csv_file.close()\n",
                "cap.release()"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "040d381c",
            "metadata": {},
            "source": [
                "## Task 3: Kalman Filter (50 points)\n",
                "\n",
                "Use the  [filterpy](https://filterpy.readthedocs.io/en/latest/kalman/KalmanFilter.html) library to implement Kalman filters that will track the cyclist and the vehicle (if present) in the video. You will need to use the detections from the previous task to initialize and run the Kalman filter. \n",
                "\n",
                "You need to deliver a video that contains the trajectory of the objects as a line that connects the pixels that the tracker indicated. You can use the `ffmpeg` command line tool and OpenCV to superpose the bounding box of the drone on the video as well as plot its trajectory. \n",
                "\n",
                "Suggest methods that you can use to address  false positives and how the tracker can help you in this regard.\n",
                "\n",
                "You will need to have one Kalman filter to track each of the required and present objects (cyclist and vehicle)."
            ]
        },
        {
            "cell_type": "markdown",
            "id": "8a2dc9f5",
            "metadata": {},
            "source": []
        },
        {
            "cell_type": "markdown",
            "id": "dfaf9ea3",
            "metadata": {},
            "source": [
                "## Extra Bonus (20 points)\n",
                "\n",
                "```{eval-rst}\n",
                ".. youtube:: https://www.youtube.com/watch?v=2hQx48U1L-Y2hQx48U1L-Y\n",
                "```\n",
                "\n",
                "The cyclist in the video goes in and out of occlusions. In addition the object is small making detections fairly problematic without finetuning and other optimizations.  Fintetuning involves using the pretrained model and training it further using images of cyclists from a training dataset such as [VisDrone](https://github.com/VisDrone/VisDrone-Dataset). At the same time,  reducing the number of classes to a much smaller number such as person & bicycle may help.  Also some 2 stage detectors may need to be further optimized in terms of parameters for small objects. See [this paper](https://www.mdpi.com/1424-8220/23/15/6887) for ideas around small object tracking. \n",
                "\n",
                "```{note}\n",
                "The extra points can only be awarded in the category of `assignments` and cannot be used to compensate for any other category such as `exams`. \n",
                "```"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.11.6"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
