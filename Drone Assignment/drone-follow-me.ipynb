{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": 40,
            "id": "dc46826e",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Run this to cleanup environment if you need to rerun the code!\n",
                "import os\n",
                "os.system(\"rm *.mp4\")\n",
                "os.system(\"rm *.txt\")\n",
                "os.system(\"rm *.csv\")\n",
                "os.system(\"rm -rf result*\")\n",
                "if os.path.isdir(\"../runs\"):\n",
                "    os.system(\"rm -rf ../runs\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "b53b6f23",
            "metadata": {},
            "source": [
                "# Drone follow me using Kalman Filters\n",
                "\n",
                "Multi-Object Tracking (MOT) is a core visual ability that humans poses to perform kinetic tasks and coordinate other tasks. The AI community has recognized the importance of MOT via a series of [competitions](https://motchallenge.net). \n",
                "\n",
                "In this assignment, the object class is `bicycle` and `car` the ability to track these objects  will be demonstrated using [Kalman Filters](https://en.wikipedia.org/wiki/Kalman_filter).  \n",
                "\n",
                "\n",
                "## Task 1: Setup your development environment and store the test video locally (10 points)\n",
                "\n",
                "Your environment must be docker based and you can use any TF2 or PT2 based docker container compatible with your environment. You can also use colab. "
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "5ba49eb5",
            "metadata": {},
            "outputs": [],
            "source": [
                "from pytube import YouTube\n",
                "YouTube('https://www.youtube.com/watch?v=WeF4wpw7w9kWeF4wpw7w9k').streams.first().download()\n",
                "YouTube('https://www.youtube.com/watch?v=2NFwY15tRtA').streams.first().download()\n",
                "YouTube('https://www.youtube.com/watch?v=5dRramZVu2Q').streams.first().download()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 42,
            "id": "2385bd73",
            "metadata": {},
            "outputs": [],
            "source": [
                "input_file1 = 'Cyclist and vehicle Tracking - 1.mp4'\n",
                "input_file2 = 'Cyclist and vehicle tracking - 2.mp4'\n",
                "input_file3 = 'Drone Tracking Video.mp4'"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "8514b0e1",
            "metadata": {},
            "source": [
                "## Task 2: Object Detection (40 points)\n",
                "\n",
                "Perform object detection on the following videos. \n",
                "\n",
                "```{eval-rst}\n",
                ".. youtube:: https://www.youtube.com/watch?v=WeF4wpw7w9kWeF4wpw7w9k\n",
                "```\n",
                "\n",
                "```{eval-rst}\n",
                ".. youtube:: https://www.youtube.com/watch?v=2NFwY15tRtA\n",
                "```\n",
                "\n",
                "```{eval-rst}\n",
                ".. youtube:: https://www.youtube.com/watch?v=5dRramZVu2Q\n",
                "```\n",
                "Split the videos into frames and use an object detector of your choice, in a framework of your choice to detect the cyclists.  "
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "8c8ca5c7",
            "metadata": {},
            "outputs": [],
            "source": [
                "from ultralytics import YOLO\n",
                "import cv2\n",
                "\n",
                "if os.path.isdir(\"./result1\") != True:\n",
                "    os.mkdir(\"./result1\")\n",
                "\n",
                "if os.path.isdir(\"./result2\") != True:\n",
                "    os.mkdir(\"./result2\")\n",
                "\n",
                "if os.path.isdir(\"./result3\") != True:\n",
                "    os.mkdir(\"./result3\")\n",
                "\n",
                "# Load the model\n",
                "# mac = yolov8n.pt\n",
                "# windows = yolov8x.pt\n",
                "model = YOLO('yolov8x.pt')\n",
                "\n",
                "cap = cv2.VideoCapture(input_file1)\n",
                "n_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
                "    \n",
                "for frame in range(n_frames):\n",
                "    ret, img = cap.read()\n",
                "    if not ret:\n",
                "        break\n",
                "\n",
                "        #results = model(frame, device=\"mps\")\n",
                "        # use mps for macbook and device=\"0\" for windows\n",
                "    results = model.predict(source=img, save=True, device=\"0\", conf=0.4)\n",
                "\n",
                "    for result in results:\n",
                "        boxes = result.boxes  # Boxes object for bounding box outputs\n",
                "        masks = result.masks  # Masks object for segmentation masks outputs\n",
                "        keypoints = result.keypoints  # Keypoints object for pose outputs\n",
                "        probs = result.probs  # Probs object for classification outputs\n",
                "        result.save(filename=f'./result1/result{frame}.jpg')  # save to disk\n",
                "        result.save_txt('result1.txt', True)\n",
                "\n",
                "cap.release()\n",
                "\n",
                "cap = cv2.VideoCapture(input_file2)\n",
                "n_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
                "    \n",
                "for frame in range(n_frames):\n",
                "    ret, img = cap.read()\n",
                "    if not ret:\n",
                "        break\n",
                "\n",
                "        #results = model(frame, device=\"mps\")\n",
                "        # use mps for macbook and device=\"0\" for windows\n",
                "    results = model.predict(source=img, save=True, device=\"0\", conf=0.4)\n",
                "\n",
                "    for result in results:\n",
                "        boxes = result.boxes  # Boxes object for bounding box outputs\n",
                "        masks = result.masks  # Masks object for segmentation masks outputs\n",
                "        keypoints = result.keypoints  # Keypoints object for pose outputs\n",
                "        probs = result.probs  # Probs object for classification outputs\n",
                "        result.save(filename=f'./result2/result{frame}.jpg')  # save to disk\n",
                "        result.save_txt('result2.txt', True)\n",
                "\n",
                "cap.release()\n",
                "\n",
                "cap = cv2.VideoCapture(input_file3)\n",
                "n_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
                "    \n",
                "for frame in range(n_frames):\n",
                "    ret, img = cap.read()\n",
                "    if not ret:\n",
                "        break\n",
                "\n",
                "        #results = model(frame, device=\"mps\")\n",
                "        # use mps for macbook and device=\"0\" for windows\n",
                "    results = model.predict(source=img, save=True, device=\"0\", conf=0.4)\n",
                "\n",
                "    for result in results:\n",
                "        boxes = result.boxes  # Boxes object for bounding box outputs\n",
                "        masks = result.masks  # Masks object for segmentation masks outputs\n",
                "        keypoints = result.keypoints  # Keypoints object for pose outputs\n",
                "        probs = result.probs  # Probs object for classification outputs\n",
                "        result.save(filename=f'./result3/result{frame}.jpg')  # save to disk\n",
                "        result.save_txt('result3.txt', True)\n",
                "\n",
                "cap.release()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "ed8cd1a3",
            "metadata": {},
            "outputs": [],
            "source": [
                "from ultralytics import YOLO\n",
                "\n",
                "# Load a pretrained YOLOv8n model\n",
                "model = YOLO('yolov8x.pt')\n",
                "\n",
                "# Define source as YouTube video URL\n",
                "source1 = input_file1\n",
                "\n",
                "# Run inference on the source\n",
                "results1 = model.predict(source1, save=True, device=\"0\", conf=0.4)\n",
                "\n",
                "# Define source as YouTube video URL\n",
                "source2 = input_file2\n",
                "\n",
                "# Run inference on the source\n",
                "results2 = model.predict(source2, save=True, device=\"0\", conf=0.4)\n",
                "\n",
                "# Define source as YouTube video URL\n",
                "source3 = input_file3\n",
                "\n",
                "# Run inference on the source\n",
                "results1 = model.predict(source3, save=True, device=\"0\", conf=0.4)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "8f560b33",
            "metadata": {},
            "outputs": [],
            "source": [
                "import subprocess\n",
                "result_file1 = '../runs/detect/predict2/Cyclist and vehicle Tracking - 1.avi'\n",
                "result_file2 = '../runs/detect/predict2/Cyclist and vehicle tracking - 2.avi'\n",
                "result_file3 = '../runs/detect/predict2/Drone Tracking Video.avi'\n",
                "\n",
                "subprocess.run(['ffmpeg', '-i', result_file1, \n",
                "                '-qscale', '0',\n",
                "                '-loglevel', 'quiet',\n",
                "                'output1.mp4'])\n",
                "subprocess.run(['ffmpeg', '-i', result_file2, \n",
                "                '-qscale', '0',\n",
                "                '-loglevel', 'quiet',\n",
                "                'output2.mp4'])\n",
                "subprocess.run(['ffmpeg', '-i', result_file3, \n",
                "                '-qscale', '0',\n",
                "                '-loglevel', 'quiet',\n",
                "                'output3.mp4'])"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "040d381c",
            "metadata": {},
            "source": [
                "## Task 3: Kalman Filter (50 points)\n",
                "\n",
                "Use the  [filterpy](https://filterpy.readthedocs.io/en/latest/kalman/KalmanFilter.html) library to implement Kalman filters that will track the cyclist and the vehicle (if present) in the video. You will need to use the detections from the previous task to initialize and run the Kalman filter. \n",
                "\n",
                "You need to deliver a video that contains the trajectory of the objects as a line that connects the pixels that the tracker indicated. You can use the `ffmpeg` command line tool and OpenCV to superpose the bounding box of the drone on the video as well as plot its trajectory. \n",
                "\n",
                "Suggest methods that you can use to address  false positives and how the tracker can help you in this regard.\n",
                "\n",
                "You will need to have one Kalman filter to track each of the required and present objects (cyclist and vehicle)."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "ff6185c5",
            "metadata": {},
            "outputs": [],
            "source": []
        },
        {
            "cell_type": "markdown",
            "id": "dfaf9ea3",
            "metadata": {},
            "source": [
                "## Extra Bonus (20 points)\n",
                "\n",
                "```{eval-rst}\n",
                ".. youtube:: https://www.youtube.com/watch?v=2hQx48U1L-Y2hQx48U1L-Y\n",
                "```\n",
                "\n",
                "The cyclist in the video goes in and out of occlusions. In addition the object is small making detections fairly problematic without finetuning and other optimizations.  Fintetuning involves using the pretrained model and training it further using images of cyclists from a training dataset such as [VisDrone](https://github.com/VisDrone/VisDrone-Dataset). At the same time,  reducing the number of classes to a much smaller number such as person & bicycle may help.  Also some 2 stage detectors may need to be further optimized in terms of parameters for small objects. See [this paper](https://www.mdpi.com/1424-8220/23/15/6887) for ideas around small object tracking. \n",
                "\n",
                "```{note}\n",
                "The extra points can only be awarded in the category of `assignments` and cannot be used to compensate for any other category such as `exams`. \n",
                "```"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.10"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
